\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx} 
\usepackage{subfigure} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}

\title{Project-II by Group Sydney}

\author{
Diego Antognini \& Jason Racine\\
EPFL \\
\texttt{diego.antognini@epfl.ch}, \texttt{jason.racine@epfl.ch} \\
}

\nipsfinalcopy 

\begin{document}

\maketitle

\begin{abstract}
This report provides a summary of the project two of the PCML class. 
\end{abstract}

\section{Data Description}

The train-data for binary classification consists of $N = 6000$ images. As input we have 2 representations of the image : \textit{the histogram of oriented gradients} (HOG) $\mathbf{X}_{hog}$ and the \textit{overFEAT ImageNet CNN features} (CNN) $\mathbf{X}_{cnn}$. Our input $\mathbf{X}$ is the concatenation of $\mathbf{X}_{hog}$ with $\mathbf{X}_{cnn}$. 

Each input sample is a vector $\mathbf{x}_n$ with dimension $D = 42273$ ($5408$ for the HOG and $36865$ for the CNN) and is the concatenation of $\mathbf{x}_n^{hog}$ with $\mathbf{x}_n^{cnn}$
The output ($\mathbf{y}$) represents the classification of the images. For the binary classification, the label $1$  represents a car, a horse or a plane and the label $2$ anything else. For the multi-class classification, the label $1$ represents a car, $2$2 a horse, $3$ a plane and $4$ anything else.

We also have test-data of size $N=XXX$ without their corresponding output. Our goal is to produce predictions for those data, as well as an approximation of the test-error using \textit{Balanced Error Rate} (BER).

\subsection{Histogram of Oriented Gradients}

Histogram of oriented gradients is a feature used in computer vision in order to detect objects. To compute it, we first need to normalize the image, compute the gradients (of each pixel) for the different color channel and take those with the largest norm. Then we decompose the image in bins of size $w \times h$. For each of those bins, we compute an histogram of the orientation of the gradients using theirs angles and weighted by theirs magnitudes. The histogram has $n$ bins from $0$ to $180$ degrees. We compute this histogram using $4$ different normalizations.

In our case, the dimensions of this feature is $13 \times 13 \times 32=5408$, where the first $13$ is the number of bins in the height, the second $13$ the number of bins in the width. Finally $32$ is $4 \times 8$ where the $4$ is the different normalizations for the histogram and $8$ the number of bins (each bin has a size of $22.5Â°$). 

%http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf
%http://vision.ucsd.edu/~pdollar/toolbox/doc/channels/hog.html
\subsection{OverFEAT ImageNet CNN features}

Those features are extracted from a convolutional network-based image features extractor OverFeat. They were trained on the ImageNet dataset (tens of millions images). The size is the output of the fifth layer of the neural network which is $1024 \times 6 \times 6 = 36864$. In our features, we also have an extra dimension which can be ruled out.

\section{Data visualization and analysis}

We first have plot the labels distribution for the binary and multi-class cases. For the binary case, we have $60.3\%$ for the label $1$ and $39.8\%$ for the second one. We have observed that there is a small imbalance between the classesl. For the multi-classes case, we obtain $16.07\%$, $19.37\%$, $24.87\%$ and $39.70\%$ for the labels $1$, $2$, $3$ and $4$. This time, the imbalance is more important, especially with the label $4$ and we do have to take care about it during the training.

We also wanted to analyze the distribution of our features, and have observed that for the features HOG, each of them has a has similar distribution as showed in Figure XXX. The values are essentially in the range $[0, 0.2]$. We are also interested about the correlation between the input and output variables. We have observed that the correlation is in the range $[-0.24 ;0.30]$, and so can conclude that features seem not to have a direct correlation between them. However, a combination of features can have a high correlation with the output but we couldn't find such combination. Finally, $\mathbf{X}_{hog}$ is rank-deficient with a rank of $3467$ instead of $5408$.

For the features CNN, first we can see that it is a sparse matrix and most of the values for each data is $0$ (around $95.5\%$). This means that for each data, a small subset is able to describe the picture. Moreover, we can observe that we are faced to the problem of $D > N$ and the solution we have adopted is PCA which will describe during the next sections. The values are in mainly in the range $[0, 64.87]$. We also wanted to see the correlation of those features with the output and have found that they are in the range $[-0.24;0.18]$, which is similar to the HOG features. Finally,  $\mathbf{X}_{cnn}$ is rank-deficient with a rank of $5997$ instead of $36865$.

\section{Techniques used}

This section is a description of the technique we have used in the project. It describes mainly how it works, why we thought it could be useful in this project.

\subsection{Principal component analysis}

As we have seen in the section "Data visualization and analysis", we are faced to the $D > N$ problem mainly with the CNN features and also the HOG features depending on how we se the $k$ for the k-fold cross validation. One way to solve this problem is to use PCA.  Basically, PCA works as follow : we compute the mean and the covariance matrix $S = \frac{1}{N}\sum_{n=1}^{N}(x_n-\bar{x})(x_n-\bar{x})^T$ of our data and then compute the $M$ eigenvectors of $S$ corresponding to the $M$ largest eigenvalues. $M$ is the number of features we have to take into account in order to minimize the reconstruction. Finally, the approximation of a data sample is $\tilde{x_n} = \sum_{i=1}^M x_{ni} u_i$. However, we couldn't compute PCA using this method, because computing the full eigenvector decomposition a matrix $D \times D$ runs in  $O(D^3)$ and D is quite large. 
We have used an alternative proposed in [REF] which allows us to compute the same eigenvectors in $O(N^3)$, which is roughly at least $230$ times faster. The tricks is to express $S = \frac{1}{N}XX^T$ and finally using $\frac{1}{N}XX^Tv_i = \lambda_i v_i$, where $v_i=Xu_i$, we obtain an eigenvector equation for the $N \times N$ matrix $S$.

The last parameter to set, is the parameter $M$ which defines the $M$ largest eigenvectors to take into consideration for our feature selection.We introduce a distortion measure $J = \frac{1}{N}\sum_{n=1}^{N}||x_n-\tilde{x_n}||^2$ which represents the mean squared distance between the original data sample and our approximation. Without going into the steps, the solution to the minimization of $J$ is $J = \sum_{i=M+1}^D \lambda_i$. 

The Figure XX shows the plot of $J$ for the HOG features and the CNN features. For the CNN, we have tried several values for $M$ and have conclude that $M_{CNN} = 150$ was a good value allowing us to compute faster results without loosing too much accuracy. The corresponding distortion might be very high compared to the HOG features, but the range of the values are not the same, CNN has values going larger than $0.2$. We tried some bigger values but we didn't get major improvements in terms of error. For the HOG features, the distortion measure seems more nice, we did the same experiments and finally set $M_{HOG}$ = $500$. 

\subsection{Neural Networks}

This is the multi-layer perceptron (MLP) we have seen in class. The neural networks used in this project comes from the given DeepLearning matlab toolbox. It uses batch sizes in order to use less memory (take the first $n$ data samples and works on them, then the next $n$ etc). We have let the default parameter ($100$). The number of epochs is simply the number of time where all the training data do a forward and a backward pass. We have put this value to $20$, in order to have a little more accuracy without loosing a lot of time, it was a good compromise because larger epochs don't mean especially better accuracy. the number of input is $M$ and output is either $2$ or $4$, depending of the type of classification (binary or multi-class). For the number of hidden layer, we have let $1$ because we didn't see improvement by adding some more. However, the number of neurons in the hidden layer has been tested and have found $1000$ for the binary classification and $700$ for the multi-class case. We also have to set up the learning rate : the optimal value we have found is $3$ for the binary case and $2$ for the multi-class one. By default, the activation function used is the \textit{tanh} one. We didn't see so much different using the \textit{sigmoid}. Finally, the algorithm uses stochastic gradient descent using a momentum of $0.5$, which might help to converge faster.

Neural networks are suitable for problems which might be difficult to describe in a mathematically manner and is robust to the noise, which is very useful in our case. However, we have to be careful with the weights initialization because neural networks are very sensitive to them, and especially with overfitting and computing the generalization error, this task is harder with neural networks.

\subsection{Support Vector Machine}

The normal support vector machine is a technique used for binary classification. The goal is to divide the space using an hyperplan in a way to maximize the margin, which represents the distance between closest members of the two classes. It is an interesting method because it has a regularization parameter which help to avoid overfitting. Another advantages is we can easily change the kernel function in order to try to find better boundaries. However, what is difficult is to find good parameters ($C$, $\gamma$). $\gamma$ is the kernel scale parameter which is related to how the data are scattered. We might have many support vectors and overfit if we set the $C$ too large and on the contrary if it is small, we might have not enough support vector and underfit. However, the best results we can obtain with support vector machine need to have a small imbalanced between the positive and negative examples. As seen in the section "Data visualization and analysis", there classes are not very well balanced as much for the binary classification as the multi-class classification. We have use the same algorithm we have seen in the lab $SMO$. 

We have used the method \textit{fitcecoc} in matlab, and we don't need to specify the parameter $C$, it is computed during the process. However, we have to set the kernel function and $\gamma$. We have k-fold cross validate and have found that the linear kernel was the best and implicitly $\gamma = 1$.

To adapt support vector machine to a multi-class classifier, we have used \textit{One Versus All} method, which divides the problem in $4$ binary classifications. We train the data with all the classifiers and at the end, each classifier says if the data is positive to its class. If there are more than one classifier with the data sample considered as positive, we use the highest confidence to associate the class. Another possible method would have been to use \textit{One Versus One} which needs $6$ classifiers in our case, and is less sensitive to the imbalance but needs more computational resource, but it didn't give better results. 

\subsection{Decision Trees}

\subsection{Random Forests}

\section{Evaluation methods}

\section{Model comparison}

\subsection{Binary classification}

\subsection{Multi-class classification}

\section{Feature transformation}

\section{Conclusion}

\section{References}

@misc{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
}

@misc{PMT,
   author = {Piotr Doll\'ar}, 
   title = {{P}iotr's {C}omputer {V}ision {M}atlab {T}oolbox ({PMT})}, 
   howpublished = {\url{http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html}} 
} 



\end{document}